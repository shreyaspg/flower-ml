import flwr as fl
import utils
from sklearn.metrics import log_loss
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from typing import Dict
import pandas as pd

def fit_round(rnd: int) -> Dict:
    """Send round number to client."""
    return {"rnd": rnd}


def get_eval_fn(model: LogisticRegression):
    """Return an evaluation function for server-side evaluation."""
    col_names=['patient_id','full_name','address','age_days','age_year','gender','height','weight','ap_hi','ap_lo','cholesterol','gluc','smoke','alco','active','cardio']
    pima = pd.read_csv("knn_data.csv",header=None, names=col_names)
    print(pima.head())

    #select features
    feature_cols = ['cholesterol', 'smoke', 'alco']
    X = pima[feature_cols]
    y = pima.active

    #Selecting train and test sets

    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=1)

    y_test= pd.to_numeric(y_test)
    # Load test data here to avoid the overhead of doing it in `evaluate` itself
#    _, (X_test, y_test) = utils.load_mnist()

    # The `evaluate` function will be called after every round
    def evaluate(parameters: fl.common.Weights):
        # Update model with the latest parameters
        utils.set_model_params(model, parameters)
        loss = log_loss(y_test, model.predict_proba(X_test))
        accuracy = model.score(X_test, y_test)
        return loss, {"accuracy": accuracy}

    return evaluate


# Start Flower server for five rounds of federated learning
if __name__ == "__main__":
    model = LogisticRegression()
    utils.set_initial_params(model)
    strategy = fl.server.strategy.FedAvg(
        min_available_clients=2,
        eval_fn=get_eval_fn(model),
        on_fit_config_fn=fit_round,
    )
    fl.server.start_server("0.0.0.0:8080", strategy=strategy, config={"num_rounds": 2})
